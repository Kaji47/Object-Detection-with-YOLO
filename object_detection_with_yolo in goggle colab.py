# -*- coding: utf-8 -*-
"""Object Detection with YOLO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bLimD2jQEoeDCNdDGpxfXpL2JZ8SNwxW
"""

import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
from glob import glob
import yaml
from google.colab.patches import cv2_imshow
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
# !pip install ultralytics
from ultralytics import YOLO

# Define paths for training and validation datasets
train_path = "/content/drive/MyDrive/Colab Notebooks/new_data/train_data"
val_path = "/content/drive/MyDrive/Colab Notebooks/new_data/val_data"

# Collect image paths
train_img = glob(f'{train_path}/*.jpg')
val_img = glob(f'{val_path}/*.jpg')


# Split training images into training and validation sets
train_img, val_img = train_test_split(train_img, test_size=0.2)

# Print the number of images found
print(f"Number of images found: {len(train_img)}")
print(f"Number of images found: {len(val_img)}")

# Define labels
labels = [
    "bike",
    "bus",
    "car",
    "motor",
    "person",
    "rider",
    "traffic light",
    "traffic sign",
    "train",
    "truck"
]

# Prepare data configuration for YOLO
data = {
    "train" : os.path.abspath(train_path),
    "val" : os.path.abspath(val_path),
    "names" : labels,
    "nc" : len(labels)
}

# Write data configuration to a YAML file
with open('data.yaml', 'w+') as f:
    yaml.safe_dump(data, f)

# Load the pre-trained YOLO model
model = YOLO('/content/drive/MyDrive/Colab Notebooks/yolov8n.pt')
model.info()

# Train the model
!yolo train data=data.yaml model=yolov8n.pt epochs=50 batch=16
#AdamW(lr=0.000714, momentum=0.9)

# Evaluate the model on the validation set
true_labels = []
pred_labels = []

for img_path in val_img:
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB

    # Get the detection results from the model
    results = model(img)
    width, height = img.shape[1], img.shape[0]

    for result in results:
        objects = result.boxes.cls
        confidences = result.boxes.conf
        for obj, conf, box in zip(objects, confidences, result.boxes.xyxyn):
            if conf < 0.5:
                continue
            obj_index = int(obj)
            if 0 <= obj_index < len(labels):
                true_labels.append(labels[obj_index])
                pred_labels.append(labels[obj_index])
            else:
                print(f"Invalid object index: {obj}")

# Calculate evaluation metrics
print(classification_report(true_labels, pred_labels))

# Visualize predictions on sample images from the validation set
n_samples = 5
val_sample = np.random.choice(val_img, size=n_samples, replace=False)

# Choose one random image from the training set
for sample_img in val_sample:
  img = cv2.imread(sample_img)
  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

#  Assuming model returns a list of dictionaries with bounding box info
  results = model(img)
  classes= model.names
  width = img.shape[1]
  height = img.shape[0]
  for result in results:
    objects = result.boxes.cls
    bboxes = result.boxes.xyxyn  # box for person shows coordination for object(calculation: coordinate/ pixels)
    confidences = result.boxes.conf
    for obj, conf, box in zip(objects, confidences, bboxes):
        conf = conf.tolist()
        if conf < 0.5:
            continue
        conf = int(conf * 100)
        x1, y1 = int(box[0] * width), int(box[1] * height)
        x2, y2 = int(box[2] * width), int(box[3] * height)
        label = classes[obj.tolist()]
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)   #bgr
        cv2.putText(img, f'{label}:({conf})%', (x1, y1-10),
                    cv2.FONT_HERSHEY_COMPLEX, 1, (255, 240, 50), 2
                    )

  cv2_imshow(img)